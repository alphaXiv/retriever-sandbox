[
  {
    "query": "Which are the best performing alternatives optimizers to the traditional ones like Adam, Momentum, and SGD?",
    "papers": [
      "2409.11321",
      "2502.16982",
      "2305.14342",
      "2405.15682",
      "2406.16793",
      "2507.20534",
      "2510.09378",
      "2403.03507"
    ]
  },
  {
    "query": "Have people tried adding entropy loss to GRPO?",
    "papers": [
      "2506.14758",
      "2504.10449",
      "2503.17287"
    ]
  },
  {
    "query": "MaxRL from Fahim Tajwar",
    "papers": [
      "2602.02710"
    ]
  },
  {
    "query": "Evolutionary policy optimization from CMU",
    "papers": [
      "2503.19037"
    ]
  },
  {
    "query": "Why is Qwen so easily able to replicate realistic chat-like behavior when RL-ing with cold start?",
    "papers": [
      "2503.18892",
      "2502.03373",
      "2504.07912",
      "2505.09388"
    ]
  },
  {
    "query": "What benchmarks does DeepSeek OCR use in its results?",
    "papers": [
      "2510.18234"
    ]
  },
  {
    "query": "What are the most commonly referenced benchmarks for testing LLM tool-use?",
    "papers": [
      "2406.18518",
      "2406.12045",
      "2504.13958"
    ]
  },
  {
    "query": "Paper from a joint collaboration between UNC and Salesforce Research that has agents improve in a self-reinforcing cycle on tasks with tools",
    "papers": [
      "2511.16043"
    ]
  },
  {
    "query": "What optimizer method was used to training Kimi K2.5?",
    "papers": [
      "2602.02276",
      "2507.20534"
    ]
  },
  {
    "query": "Which works have the best performance on MMMU pro?",
    "papers": [
      "2602.02276",
      "2505.07062",
      "2511.21631",
      "2504.07491",
      "2509.17765"
    ]
  },
  {
    "query": "What works does OlmoOCR 2 compare itself against?",
    "papers": [
      "2510.18234",
      "2510.14528",
      "2506.05218",
      "2509.22186",
      "2512.02498",
      "2506.03197",
      "2511.10390",
      "2502.18443"
    ]
  },
  {
    "query": "Papers showing comparison between using tied weights embeddings vs not for different model comparison and how does it help in convergence of said models.",
    "papers": [
      "1608.05859",
      "1909.11942",
      "2412.15115",
      "2406.07887",
      "2306.11397",
      "2505.10202"
    ]
  },
  {
    "query": "What is the best performing open-source model from the organization that put out GDPO?",
    "papers": [
      "2505.00949",
      "2601.05242"
    ]
  },
  {
    "query": "Which models perform best on Tau-Bench?",
    "papers": [
      "2508.06471",
      "2506.13585",
      "2406.12045",
      "2508.18669",
      "2601.05808",
      "2508.10925",
      "2507.20534"
    ]
  },
  {
    "query": "Which models do best on Terminal Bench 2.0?",
    "papers": [
      "2601.11868",
      "2602.02276",
      "2512.02556",
      "2602.03786",
      "2601.02780"
    ]
  },
  {
    "query": "What is the largest open-source LLM released in terms of parameter count?",
    "papers": [
      "2507.20534",
      "2510.22115",
      "2412.19437",
      "2407.21783",
      "2505.09388"
    ]
  },
  {
    "query": "Which open source models have architectures using Deepseek's sparse attention architecture?",
    "papers": [
      "2412.19437",
      "2405.04434",
      "2501.12948",
      "2510.26692",
      "2507.20534",
      "2509.01322",
      "2512.24618",
      "2601.07372",
      "2502.14837",
      "2502.07864"
    ]
  },
  {
    "query": "Joint collaboration between AMD and John Hopkins on designing a fully autonomous lab with agents",
    "papers": [
      "2501.04227"
    ]
  },
  {
    "query": "Which models score best on Humanity's Last Exam without tool usage?",
    "papers": [
      "2501.14249",
      "2507.06261",
      "2602.02276",
      "2508.06471",
      "2507.20534"
    ]
  },
  {
    "query": "Which open source models score best on Humanity's Last Exam?",
    "papers": [
      "2509.13309",
      "2511.11793",
      "2510.24701",
      "2512.02556",
      "2501.14249",
      "2508.06471",
      "2507.20534",
      "2509.06283"
    ]
  },
  {
    "query": "Find papers that propose replacing heavy human feedback aggregation with a set of natural language principles or a \"constitution\" to guide the model's self-critique and refinement process, often referred to as RLAIF.",
    "papers": [
      "2212.08073",
      "2309.00267",
      "2305.03047",
      "2303.17651"
    ]
  },
  {
    "query": "Which paper from Nvidia improves upon GRPO by decoupling the normalization of individual reward components?",
    "papers": [
      "2601.05242"
    ]
  },
  {
    "query": "What open models and benchmarks does Dr Tulu 8b compare against?",
    "papers": [
      "2511.19399",
      "2510.24701",
      "2504.21776",
      "2509.06501",
      "2503.09516",
      "2508.07976",
      "2505.09388",
      "2505.08775",
      "2509.00496",
      "2411.14199"
    ]
  },
  {
    "query": "Which other open source models does GLM 4.5 benchmark against?",
    "papers": [
      "2508.06471",
      "2505.09388",
      "2512.02556",
      "2501.12948",
      "2507.20534"
    ]
  },
  {
    "query": "Which models include vending bench results in their paper?",
    "papers": [
      "2502.15840"
    ]
  },
  {
    "query": "What is the most common open-source model used in papers that perform some type of model fine-tuning techniques?",
    "papers": [
      "2504.13837",
      "2501.12948",
      "2504.11343",
      "2504.14945",
      "2504.00698"
    ]
  },
  {
    "query": "What are some open-source, fine-tunable multimodal models with fewer than 0.5 billion parameters?",
    "papers": [
      "2504.05299",
      "2402.14289",
      "2408.01800",
      "2503.19786",
      "2504.01990"
    ]
  },
  {
    "query": "Which paper argues that a successful alignment algorithm should use on-policy sampling and negative gradients?",
    "papers": [
      "2404.14367"
    ]
  },
  {
    "query": "Paper from Xuandong Zhao that introduces RL from internal feedback?",
    "papers": [
      "2505.19590"
    ]
  },
  {
    "query": "What are good math benchmarks for evaluating an LLM's ability to do math reasoning?",
    "papers": [
      "2402.03300",
      "2402.14008",
      "2305.12524",
      "2504.11456"
    ]
  },
  {
    "query": "Good benchmarks that test long context performance of models",
    "papers": [
      "2404.06654",
      "2308.14508",
      "2412.15204",
      "2402.13718"
    ]
  },
  {
    "query": "What papers/works does Llama 3 benchmark against?",
    "papers": [
      "2407.21783",
      "2303.08774",
      "2312.11805",
      "2310.06825",
      "2503.19786",
      "2401.04088",
      "2406.11704"
    ]
  },
  {
    "query": "What normalization methods are researchers trying besides Layernorm for training LLMs?",
    "papers": [
      "1910.07467",
      "2503.10622",
      "2512.10938",
      "2410.01131",
      "2503.04598",
      "2410.05258",
      "2504.07866",
      "2503.19786"
    ]
  },
  {
    "query": "For papers that RL post-train to improve AIME scores, what base models are most popular?",
    "papers": [
      "2501.12948",
      "2503.14476",
      "2504.05118",
      "2504.16084",
      "2504.14945",
      "2504.20571",
      "2505.10425",
      "2505.00551",
      "2503.20783",
      "2504.07086"
    ]
  },
  {
    "query": "What is the best performing model from the organization that put out GSPO?",
    "papers": [
      "2507.18071",
      "2505.09388"
    ]
  },
  {
    "query": "What open models and benchmarks does WebThinker-32B-DPO compare against?",
    "papers": [
      "2504.21776",
      "2501.05366",
      "2501.12948"
    ]
  },
  {
    "query": "What is the best performing model from the organization that released the Engram paper?",
    "papers": [
      "2601.07372",
      "2501.12948",
      "2412.19437"
    ]
  },
  {
    "query": "Have people tried using LoRA/QloRA or any PEFT techniques with ES, especially when tuning large models?",
    "papers": [
      "2511.16652",
      "2509.24372",
      "2305.17333"
    ]
  },
  {
    "query": "Which paper introduces the LoRA technique for parameter-efficient fine-tuning?",
    "papers": [
      "2106.09685"
    ]
  },
  {
    "query": "What papers/methods did the Meta team utilize while post-training Llama 3?",
    "papers": [
      "2407.21783",
      "2305.18290"
    ]
  },
  {
    "query": "What are the best benchmarks for testing agents abilities to complete economically useful real-world tasks?",
    "papers": [
      "2502.12115",
      "2412.14161",
      "2403.07718",
      "2503.14499",
      "2504.02670"
    ]
  },
  {
    "query": "What series of open-source models did the organization behind the Star-Attention paper produce?",
    "papers": [
      "2411.17116",
      "2505.00949",
      "2504.03624",
      "2512.20856"
    ]
  },
  {
    "query": "Why is Qwen so easily able to replicate realistic chat-like behavior when RL-ing with cold start?",
    "papers": [
      "2504.05812",
      "2412.15115",
      "2505.09388",
      "2512.07783",
      "2501.12948"
    ]
  },
  {
    "query": "Which OCR methods do best on OmniDocBench?",
    "papers": [
      "2601.21957",
      "2511.19575",
      "2511.10390",
      "2601.20552",
      "2509.22186",
      "2510.14528",
      "2511.21631",
      "2512.02498",
      "2412.07626"
    ]
  },
  {
    "query": "Fetch me the Qwen3 technical report",
    "papers": [
      "2505.09388"
    ]
  },
  {
    "query": "Pre-trained latent video diffusion model from Nvidia",
    "papers": [
      "2501.03575",
      "2503.14492",
      "2503.15558"
    ]
  },
  {
    "query": "Which papers score best on LiveCodeBench Pro?",
    "papers": [
      "2506.11928",
      "2507.06261",
      "2506.13585",
      "2501.12948",
      "2503.19786"
    ]
  },
  {
    "query": "Which other open source models does GLM 4.5 benchmark against?",
    "papers": [
      "2508.06471",
      "2412.19437",
      "2501.12948",
      "2406.12793",
      "2407.10671"
    ]
  },
  {
    "query": "Which papers show the best results for HotpotQA?",
    "papers": [
      "2508.16153",
      "2504.03160",
      "2503.19470",
      "2503.09516",
      "2501.05366",
      "2503.05592",
      "2505.04588",
      "2508.13167"
    ]
  },
  {
    "query": "LLM as a judge can be noisy for continual learning. Are there methods that attempt to use LLMs to compare different trajectories (suggesting that pair-wise comparison could be more stable than just having an LLM assign a reward out of the blue)?",
    "papers": [
      "2505.10320",
      "2504.02495",
      "2505.14674",
      "2504.04950"
    ]
  },
  {
    "query": "Papers that assess feasbility of Muon optimizer at large scale",
    "papers": [
      "2502.16982",
      "2504.05295",
      "2510.05491",
      "2509.02046",
      "2509.01440",
      "2507.20534",
      "2505.23725"
    ]
  },
  {
    "query": "Is dropout used when training modern state-of-the-art LLMs?",
    "papers": [
      "2305.11206",
      "2306.11644",
      "2407.21783",
      "2302.10866",
      "2409.17146",
      "2503.04715",
      "2412.19437"
    ]
  },
  {
    "query": "What is that spurious rewards paper from the University of Washington?",
    "papers": [
      "2506.10947"
    ]
  },
  {
    "query": "Which paper introduces the Tiny Recursive Model (TRM)?",
    "papers": [
      "2510.04871"
    ]
  },
  {
    "query": "What are the best open-source models on SWE-bench verified?",
    "papers": [
      "2602.03411",
      "2507.20534",
      "2601.01426",
      "2601.18418",
      "2502.18449",
      "2504.07164",
      "2510.02387",
      "2509.02547"
    ]
  },
  {
    "query": "What are some types of tasks where evolutionary strategies outperform RL?",
    "papers": [
      "1703.03864",
      "2509.24372",
      "2511.16652",
      "2303.04150"
    ]
  },
  {
    "query": "Which papers show the best results for HotpotQA?",
    "papers": [
      "2508.16153",
      "2504.03160",
      "2508.13167",
      "2503.09516",
      "2503.05592",
      "2510.08558",
      "2505.04588",
      "2501.05366"
    ]
  },
  {
    "query": "What deep research benchmarks does Dr. Tulu-8B use?",
    "papers": [
      "2511.19399",
      "2505.08775",
      "2509.00496",
      "2510.21652",
      "2411.14199",
      "2510.02190"
    ]
  },
  {
    "query": "Which open-source models do best on ARC-AGI 2?",
    "papers": [
      "2512.14693",
      "2510.04871",
      "2506.21734",
      "2505.11831",
      "2509.18883"
    ]
  },
  {
    "query": "For papers that RL post-train to improve AIME scores, what datasets do they typically use?",
    "papers": [
      "2501.12948",
      "2503.14476",
      "2504.11456",
      "2505.09388",
      "2504.14945",
      "2502.17387"
    ]
  },
  {
    "query": "Work from Andrew Zhao that discovers self-play reasoning without any external data",
    "papers": [
      "2505.03335"
    ]
  },
  {
    "query": "Which models perform best on Tau-Bench?",
    "papers": [
      "2406.12045",
      "2508.06471",
      "2506.13585",
      "2507.20534",
      "2504.00698",
      "2512.20848"
    ]
  },
  {
    "query": "Identify key studies that quantify the massive CO2 emissions and energy consumption associated with training large NLP models, advocating for efficiency metrics alongside accuracy.",
    "papers": [
      "1906.02243",
      "1907.10597",
      "2211.05100",
      "2104.10350",
      "2302.13971",
      "2109.05472",
      "2311.16863",
      "2304.03271",
      "2504.17674",
      "2410.12032"
    ]
  },
  {
    "query": "Which open source works does Kimi K2.5 benchmark and compare itself against?",
    "papers": [
      "2512.02556",
      "2511.21631"
    ]
  },
  {
    "query": "Which open-source models does the MiniMax-M1 paper compare itself to?",
    "papers": [
      "2506.13585",
      "2501.12948",
      "2505.09388"
    ]
  },
  {
    "query": "What are the most popular multi-hop reasoning benchmarks?",
    "papers": [
      "1809.09600",
      "2108.00573",
      "2204.09140",
      "2503.19470",
      "2503.16416",
      "2503.09516",
      "2501.05366",
      "2503.21729"
    ]
  },
  {
    "query": "Paper introducing Mamba SSMs from CMU authors",
    "papers": [
      "2312.00752"
    ]
  },
  {
    "query": "Papers analyzing the impact of process rewards when doing GRPO",
    "papers": [
      "2502.01456",
      "2511.10279",
      "2506.11902",
      "2509.21240",
      "2509.21154",
      "2503.12937"
    ]
  },
  {
    "query": "What is the latest open source model put out by the organization that created Code World Model (CWM)?",
    "papers": [
      "2601.11659"
    ]
  },
  {
    "query": "Paper from OpenAI exploring evolutionary strategies as a viable alternative to RL",
    "papers": [
      "1703.03864"
    ]
  },
  {
    "query": "What post-training methods are used by models that benchmark against Tau-bench?",
    "papers": [
      "2508.06471",
      "2506.13585",
      "2507.20534",
      "2508.10925",
      "2507.11407",
      "2501.12948"
    ]
  },
  {
    "query": "Which open-source models use the Muon optimizer?",
    "papers": [
      "2502.16982",
      "2507.20534",
      "2504.07491",
      "2510.26692",
      "2504.18425",
      "2602.02276",
      "2512.14693",
      "2601.07372"
    ]
  },
  {
    "query": "Paper from ETH Zurich on doing RL with self-distillation",
    "papers": [
      "2601.20802"
    ]
  },
  {
    "query": "Which models does Deepseek-V3 benchmark and compare itself against?",
    "papers": [
      "2412.19437",
      "2412.15115"
    ]
  },
  {
    "query": "Papers examining the impact of mid-training in between pre-training and RL fine-tuning",
    "papers": [
      "2512.07783",
      "2506.20512",
      "2510.24701",
      "2508.06471",
      "2509.18883"
    ]
  }
]