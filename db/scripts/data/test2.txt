What are the most popular ways to create synthetic, high-quality math datasets?
How can I guide attention-based models to focus on part of some objects in images and neglect the background?
How can we design reinforcement learning methods that don't depend on counterfactual reasoning about alternative actions?
What are the most suitable and progressive metrics for ASR (in IT domain)?
How to use off-policy samples in RL, especially with LLMs?
What are techniques for sample-efficient pre-training of small language models?
What is the most popular approach for multi-vector retrieval for RAG at the moment?
Why there is no open source models that is using next scale prediction for autoregressive image generation ? Is it a problem with the idea or there is something missing?
What do the KoLeo regularizer and Gram anchor loss do in DINOv3? What are their equations, what is their purpose, and why are they necessary?
How can I edit factual knowledge in LLMs with MOE architectures?
Is there any work that uses DPO to change the reasoning language a model uses?
My goal is to fine-tune a model on a particular domain database for text to SQL task. However, the challenge is to finetune the model with very limited domain data. I dont need suggestions on the dataset enhancement side but making efficient training which captures all available information and converges efficiently(especially using better LoRA initialization)
For efficient inference of LLMs and MLLMs, I know many techniques such as token pruning, model compression (distillation, low-rank compression, pruning), efficient attention (flash attention, linear attention), the speedup of decoding (speculative decoding). Are there any other key techniques? 
Find new audio deepfake datasets from 2025 or later.
Why does training a model with AdamW sometimes cause the loss to reach 0 then spike back up quickly then decrease back down slowly?
In the case of multi-hop question answering, where two tasks are chained together, do circuits for both tasks compose?
What is the minimum storage/memory requirement of LoRA and FedLoRA?
What is the best method to make transformer model predict continuous vectors in one step?
I want to implement single channel speech enhancement audio front end, can you help me search what is the current sota?
