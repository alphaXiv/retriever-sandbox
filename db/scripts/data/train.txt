What are tricks for converging during pre-training that popular open source models use?
What are tricks to improve stability during post-training?
What's the typical ratio of learning rates between pre-training and SFT fine-tuning for LLMs?
Which OCR methods do best on OmniDocBench?
What techniques are used to optimize LLMs for inference on local devices (phones, laptops, etc)?
What are the common techniques to extend the context window of an LLM that was using RoPE embeddings?
What are the best positional embedding techniques for LLMs?
When does adding a KL penalty with the reference policy help when RL fine-tuning?
What are some strategies for maximizing GPU utilization and minimizing data staleness when doing distributed RL fine-tuning of LLMs?
What are the most important and trusted benchmarks for evaluating OCR models?
What are the most important benchmarks for evaluating LLM agents in mulit-turn long horizon settings?
What are good methods for dealing with extremely long context lengths with LLMs?
What are common techniques for improving LLM pre-training instability?
What are specific regularization techniques for reducing LLM pre-training instability?
What are good benchmarks to assess LLM's tool calling abilities?
What's a good preference optimization algorithm to use if instead of having paired preference data I have raw responses like upvote/downvote from users?
What architectural changes can I make to improve convergence when training my model?
How does sequence packing affect model accuracy for LLMs during Supervised Fine Tuning?
What are the three most important hyperparameters that influence LLM preference optimization fine-tuning specifically?
Which papers use transformers in a recursive architecture to solve puzzles?
What techniques exist for when I want to fine-tune my LLM with RL but I don't have easily verifiable rewards?
What are techniques to mitigate reward-hacking when RL fine-tuning LLMs for reasoning?
What are some good datasets to do SFT LLM post-training on to learn reasoning?
What is currently the best performing model (both open source and closed source) on the multi-turn benchmark Tau bench?
What are some important considerations when doing RL fine-tuning for agents in a multi-turn setting as opposed to just single-turn envs?
What are popular optimization objectives for RL fine-tuning LLMs today?
Which open-source LLM is best to do RL fine-tuning on top of?
What are the best benchmarks to test an LLM in its ability to do "deep research"?
What is more prone to inducing catastrophic forgetting in LLMs: supervised fine-tuning or RL fine-tuning?
Which factor plays a larger role in mitigating catastrophic forgetting for RL fine-tuning LLMs: the KL divergence term or the usage of on-policy data?
Describe the pareto frontier that RL and SFT fine-tuning for LLMs sit on. What are the tradeoffs of each method?
What learning rate schedules work best for RL post-training LLMs?
What are typical batch sizes, number of prompts, and number of rollouts per prompt used during GRPO training?
What are new RL post-training algorithms to address model collapse?
Why is Qwen so easily able to replicate realistic chat-like behavior when RL-ing with cold start?
What improvements can be made to GRPO to improve stability when RL fine-tuning MOE models?
What scale of reward should I provide in RLVR for LLM-finetuning with PPO? Especially when introducing something like format rewards or additional signal besides binary
How can I encourage my LLM to actually make tool calls when RL fine-tuning it for a specific task?
What is the impact of introducing negative gradients when doing alignment fine-tuning for LLMs?
Is sample reuse ok when doing alignment fine-tuning for LLMs?
Is DPO superior to PPO for LLM Alignment?
When fine-tuning for alignment how do offline, semi-online, and online DPO compare with each other?
What are some tricks to stabilize training when RL fine-tuning a large MoE model?
Which, if any, popular open source models adopt sliding window attention?
Which, if any, popular open source models train with FP8?
What are good math benchmarks for evaluating an LLM's ability to do math reasoning?
In attention-based architectures and models, where are the common placements of the normalization layer within an attention block?
What is the largest open-source LLM released in terms of parameter count?
Is dropout used when training modern state-of-the-art LLMs?
What do LLM architectures use instead of GELU these days for activations?
What normalization methods are researchers trying besides Layernorm for training LLMs?
Which paper argues that a successful alignment algorithm should use on-policy sampling and negative gradients?
Which papers have done comprehensive studies comparing DPO to PPO for alignment?
What are some simulation benchmarks for the Franka robot arm?
What is a good pre-trained base model to fine-tune on top of for VLA tasks?
What are works that fine-tune video models specifically for use as VLA control models?
Other than MoE, what architecture changes to the Transformer are used in training frontier LLMs today?
What are some continual learning strategies that actually involve updating weights at test-time instead of providing scaffolding?
What are some continual learning strategies that do not update weights at test-time?
What architectures do foundation models for robotics use?
How do VLA models generate actions from vision-language features?
How do VLA models represent actions across different robot embodiments?
Do today's SOTA VLA models support cross-robot-platform action spaces? If so, how?
How are researchers addressing the problem of models even from different families producing homogenous, non-diverse content?
How do multimodal model architectures handle the different embedding spaces of images and text? Do images get treated as tokens or are they handled separately in the attention layer?
How are positional embeddings assigned to multimodal architectures that tokenize images?
What to be mindful of when introducing MoE to a multimodal model?
Which open multimodal models uses the SigLIP image encoder and cross-attention between image and text modalities?
Are there any multimodal architectures that use the next-token-prediction paradigm for generating images, instead of diffusion?
What differentiates each stage in multi-stage RL training setups?
When RL fine-tuning a retrieval agent that has access to tools, how should I shape the rewards? Are outcome-based rewards sufficient or do I need to add process-oriented rewards centered around tool-query quality, etc?
Have people tried using intermediate rewards with GRPO? I don't know how that would look like
I am training a multi-turn agent RL policy for multi-hop search. One issue is, while it is correctly using the search tools a lot, the queries are not diverse. Someone told me that I should embed the tool called queries and do cosine similarity to punish similar queries. Do any papers actually propose or do this?
When fine-tuning a Qwen model for multi-hop search, does it make more sense to fine-tune with thinking enabled or disabled?
Find the preference optimization method that simplifies DPO by removing the need for a reference model entirely, relying instead on the average log-probability margin between winning and losing responses.
Identify key studies that quantify the massive CO2 emissions and energy consumption associated with training large NLP models, advocating for efficiency metrics alongside accuracy.
I am looking for a specific architectural innovation that replaces fixed activation functions in MLPs with learnable activation functions on edges
Find techniques that allow a language model to perform "internal" or "implicit" reasoning by generating thought vectors or pause tokens that are not outputted to the user, effectively "thinking" before speaking.
I need papers that evaluate whether sequence-to-sequence models can generalize to new combinations of known primitives (systematicity), specifically using benchmarks designed to test compositional skills.
Find frameworks that abstract away manual prompt engineering by treating LM pipelines as declarative programs that can be "compiled" and automatically optimized using bootstrapping or textual gradients.
I want to see the specific study showing that LLM performance degrades significantly when relevant information is located in the middle of a long context window, creating a "U-shaped" performance curve.
Which paper revisited neural scaling laws to determine the optimal allocation of compute budget between model size and training tokens, establishing the "20 tokens per parameter" rule of thumb?
Find research on "activation engineering" techniques that steer a model's behavior (e.g., toward honesty) at inference time by adding specific vectors to the internal hidden states rather than updating weights.
I'm looking for plug-and-play methods for diffusion models that decouple cross-attention mechanisms to allow for consistent identity generation using a reference image, specifically without requiring the computationally expensive process of fine-tuning or training a LoRA.
Identify papers that connect a pre-trained vision encoder (like CLIP) to a large language model using a simple projection layer to enable visual instruction tuning, effectively treating image patches as foreign language tokens.
Find papers that propose replacing heavy human feedback aggregation with a set of natural language principles or a "constitution" to guide the model's self-critique and refinement process, often referred to as RLAIF.
I am looking for methods that allow a weak LLM to become a strong one by engaging in a zero-sum game against its previous iteration, effectively removing the need for external human-annotated pairs during fine-tuning.
Find papers that discuss "jailbreaking" LLMs by automatically generating adversarial suffixes.
What papers propose a method to align language models using preference data without a separate reward model (skipping the RL step)?
What papers discuss "Reflexion" where agents verbally reinforce themselves to improve performance on subsequent trials?
Are there papers that benchmark the ability of LLMs to use tools via API calls?
What frameworks allow for building multi-agent conversation systems where agents can be assigned specific roles like "coder" or "critic"?
Find papers that simulate a town of generative agents interacting socially with each other.
What papers introduce an open-ended embodied agent that learns to play Minecraft without human demonstrations?
Are there papers that introduce a framework for LLMs to interact with external code interpreters to solve math problems?
What papers discuss the phenomena where LLMs fail to deduce "B is A" after learning "A is B"?
Find papers that frame prompt optimization as a gradient descent problem over discrete tokens.
Which paper finds that the average of the gradients weighted over successful trajectories is an unbiased estimator of the Nth approximation of the maximum likelihood gradient?
Which paper(s) compare simple rejection-sampling frameworks for LLM reasoning with more advanced techniques like GRPO and iterative dpo?
Which paper(s) focus on examining LLMs abilities to reason in anticipatory games like deal or no deal, prisoner's dilemma, etc?
Which paper draws connections between LLM RL with binary rewards to transformations like log loss and arcsine of the square root?
Which papers provide a benchmark that attempts to test frontier model performance on tasks that are deemed truly economically useful?
Which papers provide scaffolding for agents to do autonomous data science?
Which paper proposes recomputing log probs with the same model + feedback over the original attempt to get token-level advantage when learning with RL?
Paper where a model self-evolves by proposing and solving its own code reasoning tasks through abduction, deduction, and induction without any external supervised data
Methods from NVIDIA to improve reasoning with longer-horizon RL training for LLMs
Are there studies that assert for RL fine-tuning the data needs to be at the boundary of difficult but not too difficult and also claim the importance of mid-training?
Help me find the paper that was about training video models using the compression that already is used to separate time it was very recent
Summarize relevant work in using LLM as a Judge for detecting novelty.
Help me find papers on robot navigation using active inference
How can reinforcement learning be applied to optimize GPU operations such as scheduling, memory management, and kernel execution?
What are the current research directions and methodologies for integrating causal inference into Joint-Embedding Predictive Architectures (Causal JEPA)?
Summarize the research and techniques for using reinforcement learning as a pre-training objective
What are the key features and technical contributions of ABIDES and ABIDES-MARL for multi-agent reinforcement learning and high-fidelity market simulation in limit order books?
What are the less-explored research directions and problems in video world models, beyond extensively studied areas like memory and physics-awareness?
Find papers that related or exprimented models with RAVIR dataset
Automated Evaluation with Rubric-Based Scoring for comparing reference and student answer
What are the current benchmarks for evaluating privacy in Vision Language Models (VLMs)?
Explain the SEARCHD framework for advanced retrieval with text generation using large language models and cross-encoding re-ranking
What are efficient credit assignment methods for reinforcement learning (RL) and Reinforcement Learning from Verifiable Rewards (RLVR), particularly in the context of self-distillation?
Find articles on calculating news intensity using news embeddings and identify the types of features LLMs can extract from news headlines.
Find research papers on using optical computing techniques to train digital models and recent developments in hardware for computing
What are the best state-of-the-art fast summarizers with large context windows for mobile deployment on an iPhone, and how do they compare to BERT-based models?
Which LoRA variants insert a new matrix between the A and B decomposition matrices?
Provide a recent survey on synthetic data generation techniques and applications.
Self-play RL evolution
What are the most recent research papers on latent reasoning?
What are the current methods and applications for using Vision Language Models (VLMs) in traffic signal control?
Multi-agent RL for control of fixed wing UAVs
What is the state of the art and current progress in autonomous mathematics research?
Explain the integration and impact of attention sinks on native Mixture of Experts (MoE) architectures.
What are some recent research papers and studies regarding the application of AI agents in the medical field?
MoE LLM Post-Training quantization
Reinforcement Learning via Self-Distillation
What are some open-source, fine-tunable multimodal models with fewer than 0.5 billion parameters?
What is that paper where Nathan Lambert is one of the authors that suggests if you fine-tuning Qwen with random rewards evals can go up?
making databases faster with llm evolutionary sampling
Compare the paper 'RLP: Reinforcement as a Pretraining Objective' with 'STaR' to determine if it is a continuation, and explain how information gain is defined and calculated in RLP
Which paper implements a soft overlong penalty and noticed that truncation of long reasoning traces was inducing bias?
Which paper suggests that the token-level importance weighting of GRPO is misguided and that it instead needs to be done at the sequence level?
Which paper utilizes an evolutionary framework on top of an existing LLM to discover new SOTA algorithms?
Agentic frameworks for autonomously generating code repositories from scientific papers
Linear alternatives to standard transformer architectures
Which paper maintains log N memory states to reduce inference of a token to O (log N)
Which paper RL tunes algorithms to create high-quality fine-tuning signal for self-edits to better learn?
Are there any papers showing that evolutionary strategies are more robust than RL across different base model families?
Have people tried gradient-free optimization methods that have been successfully applied to LLM fine-tuning?
Have people tried using evolutionary strategies when fine-tuning large LLMs (models greater than 30B parameters)?
What are some types of tasks where evolutionary strategies outperform RL?
Are there any papers showing the trade-offs in speed between evolutionary strategies and RL when fine-tuning LLMs at scale?
Have people tried doing preference tuning like RLHF or DPO etc techniques using ES?
How data efficient ES is compared to RL techniques like GRPO specifically?
How effective is fine tuning non-instruct or base models using ES than something like vanilla GRPO or variants of the same?
ES for continual learning, I am thinking of using ES because of how data efficient it could be so using it to learn a task with a few examples on the fly can be huge so any papers showing that?
Have people tried using LoRA/QloRA or any PEFT techniques with ES, especially when tuning large models?
Papers showing comparison between various gradient free optimization techniques, not limited to current ES strategies paper for tuning LLMs  and others for training neural nets?
I think wrapping rewards with an optimizer would be better in terms of convergence for ES tuning of neural nets instead of using base reward multiplied by learning rate, any papers doing that?
Are there any studies showing the use of QK norm in attention calculation vs not using them at all for training vision language models? 
How does attention calculation get affected by the use of unimodal vs multi modal data mixtures when training models in terms of saturated probability distribution?
Papers showing comparison between using tied weights embeddings vs not for different model comparison and how does it help in convergence of said models.