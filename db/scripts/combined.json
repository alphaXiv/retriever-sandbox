[
  {
    "query": "What techniques exist for when I want to fine-tune my LLM with RL but I don't have easily verifiable rewards?",
    "papers": [
      "2501.12948",
      "2305.18290",
      "2212.08073",
      "2504.04736",
      "2402.03300",
      "2504.16084",
      "2504.02495",
      "2505.03335",
      "2504.11343",
      "2505.02387"
    ]
  },
  {
    "query": "What are common techniques for improving LLM pre-training instability?",
    "papers": [
      "2412.19437",
      "2504.07866",
      "2501.00656",
      "2503.10622",
      "2507.20534",
      "2309.14322",
      "2202.08906",
      "2502.16982",
      "2410.16682",
      "2405.18710"
    ]
  },
  {
    "query": "What's the typical ratio of learning rates between pre-training and SFT fine-tuning for LLMs?",
    "papers": [
      "2501.12948",
      "2412.19437",
      "2504.12285",
      "2407.21783",
      "2203.02155",
      "2503.20680",
      "2504.16072",
      "2503.22020"
    ]
  },
  {
    "query": "What are specific regularization techniques for reducing LLM pre-training instability?",
    "papers": [
      "2504.07866",
      "2503.10622",
      "2507.20534",
      "2503.05139",
      "2509.01322",
      "2504.12285",
      "2501.00656",
      "2502.05171"
    ]
  },
  {
    "query": "What are some strategies for maximizing GPU utilization and minimizing data staleness when doing distributed RL fine-tuning of LLMs?",
    "papers": [
      "2409.19256",
      "2405.11143",
      "2503.18929",
      "2505.24298",
      "2507.01663",
      "2510.12633",
      "2508.18588",
      "2511.16665",
      "2511.14617",
      "2601.10079"
    ]
  },
  {
    "query": "What are the most important and trusted benchmarks for evaluating OCR models?",
    "papers": [
      "2305.07895",
      "1912.13318",
      "2504.07491",
      "2502.13923",
      "1904.01906",
      "2411.15858",
      "2502.08417",
      "2410.21169"
    ]
  },
  {
    "query": "What's a good preference optimization algorithm to use if instead of having paired preference data I have raw responses like upvote/downvote from users?",
    "papers": [
      "2402.01306",
      "2505.14946",
      "2410.04166",
      "2408.15549",
      "2304.06767",
      "2305.18290",
      "2405.14734",
      "2407.16216"
    ]
  },
  {
    "query": "Which papers use transformers in a recursive architecture to solve puzzles?",
    "papers": [
      "2506.21734",
      "2510.04871",
      "2601.10679",
      "2502.05171",
      "2510.25741",
      "2503.14337",
      "2510.07358",
      "2507.10524",
      "2402.14083",
      "2512.14693"
    ]
  },
  {
    "query": "How does sequence packing affect model accuracy for LLMs during Supervised Fine Tuning?",
    "papers": [
      "2408.09327",
      "2107.02027",
      "2407.09105",
      "2504.10462",
      "2412.19437",
      "2503.17407",
      "2504.00595",
      "2601.02609"
    ]
  },
  {
    "query": "Which open-source LLM is best to do RL fine-tuning on top of?",
    "papers": [
      "2503.14476",
      "2503.18892",
      "2501.12948",
      "2504.11343",
      "2504.13837",
      "2502.21321",
      "2504.14945",
      "2504.04736"
    ]
  },
  {
    "query": "What are typical batch sizes, number of prompts, and number of rollouts per prompt used during GRPO training?",
    "papers": [
      "2501.12948",
      "2503.14476",
      "2504.14945",
      "2504.16084",
      "2503.18892",
      "2504.18458",
      "2504.13958",
      "2503.09516"
    ]
  },
  {
    "query": "What are the common techniques to extend the context window of an LLM that was using RoPE embeddings?",
    "papers": [
      "2309.00071",
      "2402.13753",
      "2401.01325",
      "2402.17463",
      "2310.16450",
      "2309.17453",
      "2503.17407",
      "2504.06214"
    ]
  },
  {
    "query": "What are popular optimization objectives for RL fine-tuning LLMs today?",
    "papers": [
      "2501.12948",
      "2502.21321",
      "2402.03300",
      "2405.14734",
      "2504.11343",
      "2504.04736",
      "2504.16828",
      "2505.03335"
    ]
  },
  {
    "query": "What are the three most important hyperparameters that influence LLM preference optimization fine-tuning specifically?",
    "papers": [
      "2305.18290",
      "2405.14734",
      "2407.08639",
      "2501.12948",
      "2408.13296",
      "2402.14740",
      "2505.22617",
      "2504.07912"
    ]
  },
  {
    "query": "What are some important considerations when doing RL fine-tuning for agents in a multi-turn setting as opposed to just single-turn envs?",
    "papers": [
      "2503.15478",
      "2402.19446",
      "2504.04717",
      "2509.02547",
      "2510.01132",
      "2505.11821",
      "2505.10978",
      "2509.02544",
      "2509.08827",
      "2501.09686"
    ]
  },
  {
    "query": "What improvements can be made to GRPO to improve stability when RL fine-tuning MOE models?",
    "papers": [
      "2507.18071",
      "2512.01374",
      "2510.11370",
      "2512.02556",
      "2511.20347",
      "2503.14476",
      "2510.26788",
      "2510.23027"
    ]
  },
  {
    "query": "Which OCR methods do best on OmniDocBench?",
    "papers": [
      "2511.19575",
      "2510.14528",
      "2509.22186",
      "2511.21631",
      "2502.13923",
      "2506.05218",
      "2511.10390",
      "2412.07626"
    ]
  },
  {
    "query": "What techniques are used to optimize LLMs for inference on local devices (phones, laptops, etc)?",
    "papers": [
      "2306.00978",
      "2505.02309",
      "2504.16266",
      "2504.08378",
      "2506.07900",
      "2504.12285",
      "2309.06180",
      "2502.11089",
      "2504.13471",
      "2504.19720"
    ]
  },
  {
    "query": "Which factor plays a larger role in mitigating catastrophic forgetting for RL fine-tuning LLMs: the KL divergence term or the usage of on-policy data?",
    "papers": [
      "2510.18874",
      "2509.04259",
      "2505.11711",
      "2512.13607",
      "2509.06948",
      "2512.21852",
      "2507.00432",
      "2508.16546",
      "2501.17161",
      "2505.13026"
    ]
  },
  {
    "query": "What are good methods for dealing with extremely long context lengths with LLMs?",
    "papers": [
      "2501.00663",
      "2312.00752",
      "2503.18893",
      "2502.11089",
      "2504.13173",
      "2504.03624",
      "2503.14456",
      "2404.10981",
      "2504.06261",
      "2504.05298"
    ]
  },
  {
    "query": "When does adding a KL penalty with the reference policy help when RL fine-tuning?",
    "papers": [
      "2502.21321",
      "2503.24290",
      "2505.00551",
      "2504.13837",
      "2405.14734",
      "2509.04259",
      "2503.14476",
      "2505.17667"
    ]
  },
  {
    "query": "What are the best positional embedding techniques for LLMs?",
    "papers": [
      "2104.09864",
      "2309.00071",
      "2402.13753",
      "2503.17407",
      "2305.19466",
      "2410.06205",
      "2504.10462",
      "2504.07491"
    ]
  },
  {
    "query": "What are new RL post-training algorithms to address model collapse?",
    "papers": [
      "2503.14476",
      "2505.24864",
      "2509.15207",
      "2501.18101",
      "2505.22617",
      "2504.05118",
      "2503.15477",
      "2503.06072",
      "2505.03335"
    ]
  },
  {
    "query": "What are some good datasets to do SFT LLM post-training on to learn reasoning?",
    "papers": [
      "2501.12948",
      "2504.16891",
      "2505.03335",
      "2502.13124",
      "2504.04736",
      "2504.14945",
      "2504.09037",
      "2502.21321",
      "2503.16419",
      "2503.19470"
    ]
  },
  {
    "query": "What are the best benchmarks to test an LLM in its ability to do \"deep research\"?",
    "papers": [
      "2503.16416",
      "2504.03160",
      "2504.12516",
      "2510.24701",
      "2512.20491",
      "2504.02670",
      "2503.24235",
      "2501.05366",
      "2504.19678"
    ]
  },
  {
    "query": "What learning rate schedules work best for RL post-training LLMs?",
    "papers": [
      "2501.12948",
      "2504.16084",
      "2503.14476",
      "2504.15275",
      "2505.00551",
      "2203.02155",
      "2510.13786",
      "2503.18866"
    ]
  },
  {
    "query": "Why is Qwen so easily able to replicate realistic chat-like behavior when RL-ing with cold start?",
    "papers": [
      "2503.18892",
      "2503.01307",
      "2504.07912",
      "2504.13828",
      "2501.12948",
      "2509.20357",
      "2503.20783",
      "2505.09388"
    ]
  },
  {
    "query": "Describe the pareto frontier that RL and SFT fine-tuning for LLMs sit on. What are the tradeoffs of each method?",
    "papers": [
      "2501.17161",
      "2509.04259",
      "2506.07527",
      "2508.16546",
      "2509.06948",
      "2501.12948",
      "2502.21321",
      "2509.04419"
    ]
  },
  {
    "query": "What architectural changes can I make to improve convergence when training my model?",
    "papers": [
      "2503.10622",
      "1910.07467",
      "1512.03385",
      "2512.24880",
      "2504.07866",
      "1606.08415",
      "2002.05202",
      "2503.04598",
      "2410.06940",
      "2503.23016"
    ]
  },
  {
    "query": "What are the most important benchmarks for evaluating LLM agents in mulit-turn long horizon settings?",
    "papers": [
      "2503.16416",
      "2507.21046",
      "2509.02547",
      "2504.19678",
      "2503.24235",
      "2503.13657",
      "2504.01990",
      "2504.04736"
    ]
  },
  {
    "query": "What are good benchmarks to assess LLM's tool calling abilities?",
    "papers": [
      "2503.16416",
      "2504.13958",
      "2406.18518",
      "2406.12045",
      "2504.04736",
      "2507.21046",
      "2505.00675",
      "2504.00698"
    ]
  },
  {
    "query": "What is more prone to inducing catastrophic forgetting in LLMs: supervised fine-tuning or RL fine-tuning?",
    "papers": [
      "2509.04259",
      "2507.00432",
      "2510.18874",
      "2601.02151",
      "2509.12235",
      "2506.19767",
      "2509.06948",
      "2512.17636",
      "2509.08827"
    ]
  },
  {
    "query": "What is currently the best performing model (both open source and closed source) on the multi-turn benchmark Tau bench?",
    "papers": ["2508.06471", "2507.20534", "2506.13585", "2509.17567", "2510.08558"]
  },
  {
    "query": "What are techniques to mitigate reward-hacking when RL fine-tuning LLMs for reasoning?",
    "papers": [
      "2501.12948",
      "2503.14476",
      "2504.16828",
      "2505.00551",
      "2502.21321",
      "2504.05118",
      "2504.15275",
      "2505.19590",
      "2504.11343",
      "2503.16419"
    ]
  },
  {
    "query": "What are tricks to improve stability during post-training?",
    "papers": [
      "2503.14476",
      "2508.08221",
      "2504.11343",
      "2502.21321",
      "2504.13828",
      "2503.24235",
      "2505.00551",
      "2504.05118",
      "2510.13786",
      "2503.10460"
    ]
  },
  {
    "query": "What are tricks for converging during pre-training that popular open source models use?",
    "papers": [
      "2412.19437",
      "2407.21783",
      "2504.07866",
      "2507.20534",
      "2503.10622",
      "2504.12285",
      "2504.00698",
      "2305.14342",
      "2503.05139",
      "2505.07796"
    ]
  },
  {
    "query": "What are some continual learning strategies that do not update weights at test-time?",
    "papers": [
      "2302.00487",
      "2504.07448",
      "2403.12030",
      "2503.20612",
      "2501.07278",
      "2504.07097",
      "2504.13822",
      "2305.19270",
      "2408.07666",
      "2510.15103"
    ]
  },
  {
    "query": "What do LLM architectures use instead of GELU these days for activations?",
    "papers": [
      "2407.21783",
      "2002.05202",
      "2412.19437",
      "2412.15115",
      "2504.12285",
      "2501.09223",
      "2412.13663",
      "2504.03624"
    ]
  },
  {
    "query": "How can I encourage my LLM to actually make tool calls when RL fine-tuning it for a specific task?",
    "papers": [
      "2504.04736",
      "2504.13958",
      "2504.14870",
      "2503.09516",
      "2509.02547",
      "2510.11701",
      "2503.23383",
      "2510.08558"
    ]
  },
  {
    "query": "How do VLA models represent actions across different robot embodiments?",
    "papers": [
      "2507.01925",
      "2406.09246",
      "2505.06111",
      "2503.14734",
      "2410.07864",
      "2503.10631",
      "2503.06669",
      "2307.15818",
      "2410.24164",
      "2505.04769"
    ]
  },
  {
    "query": "Which papers have done comprehensive studies comparing DPO to PPO for alignment?",
    "papers": [
      "2406.09279",
      "2509.20357",
      "2403.04642",
      "2405.14734",
      "2411.15124",
      "2402.03300",
      "2404.14367",
      "2506.21495"
    ]
  },
  {
    "query": "How do multimodal model architectures handle the different embedding spaces of images and text? Do images get treated as tokens or are they handled separately in the attention layer?",
    "papers": [
      "2504.10462",
      "2504.05299",
      "2103.00020",
      "2301.12597",
      "2504.07951",
      "2504.07491",
      "2502.13923",
      "2504.02477"
    ]
  },
  {
    "query": "Is sample reuse ok when doing alignment fine-tuning for LLMs?",
    "papers": [
      "2404.14367",
      "2501.17161",
      "2506.21495",
      "2305.18290",
      "2303.18223",
      "2503.06072",
      "2509.08827",
      "2503.02269"
    ]
  },
  {
    "query": "What strategies do code-generating agent frameworks use to manage exploding context windows?",
    "papers": [
      "2512.24601",
      "2512.07921",
      "2509.13313",
      "2505.00675",
      "2503.22625",
      "2505.10468",
      "2512.13564",
      "2510.11967"
    ]
  },
  {
    "query": "When fine-tuning a Qwen model for multi-hop search, does it make more sense to fine-tune with thinking enabled or disabled?",
    "papers": [
      "2505.09388",
      "2503.19470",
      "2504.03160",
      "2504.09858",
      "2501.12948",
      "2503.09516",
      "2501.05366",
      "2505.10425"
    ]
  },
  {
    "query": "What normalization methods are researchers trying besides Layernorm for training LLMs?",
    "papers": [
      "2503.10622",
      "1910.07467",
      "2503.04598",
      "2412.13795",
      "2203.00555",
      "2512.10938",
      "2504.07866",
      "2010.04245",
      "2410.05258"
    ]
  },
  {
    "query": "I am training a multi-turn agent RL policy for multi-hop search. One issue is, while it is correctly using the search tools a lot, the queries are not diverse. Someone told me that I should embed the tool called queries and do cosine similarity to punish similar queries. Do any papers actually propose or do this?",
    "papers": [
      "2509.10446",
      "2505.15107",
      "2410.23214",
      "2509.06733",
      "2512.16301",
      "2503.09516",
      "2501.18101",
      "2510.16724"
    ]
  },
  {
    "query": "What are some simulation benchmarks for the Franka robot arm?",
    "papers": [
      "2306.03310",
      "2406.02523",
      "2405.05941",
      "2503.10631",
      "2502.19645",
      "2009.12293",
      "2410.15959",
      "2503.14734",
      "2510.10903"
    ]
  },
  {
    "query": "When fine-tuning for alignment how do offline, semi-online, and online DPO compare with each other?",
    "papers": [
      "2506.21495",
      "2404.14367",
      "2503.12854",
      "2503.01067",
      "2405.14734",
      "2504.11343",
      "2502.21321",
      "2503.06072"
    ]
  },
  {
    "query": "Which open multimodal models uses the SigLIP image encoder and cross-attention between image and text modalities?",
    "papers": [
      "2407.21783",
      "2405.02246",
      "2204.14198",
      "2504.05299",
      "2503.19786",
      "2405.01483",
      "2502.14786"
    ]
  },
  {
    "query": "How are positional embeddings assigned to multimodal architectures that tokenize images?",
    "papers": [
      "2502.13923",
      "2511.21631",
      "2504.10462",
      "2010.11929",
      "2504.13181",
      "2504.07491",
      "2505.07062",
      "2103.00020",
      "2409.12191"
    ]
  },
  {
    "query": "What to be mindful of when introducing MoE to a multimodal model?",
    "papers": [
      "2504.07951",
      "2407.21770",
      "2504.07491",
      "2505.07062",
      "2508.09779",
      "2509.03498",
      "2405.11273",
      "2503.07137",
      "2412.19437",
      "2503.20680"
    ]
  },
  {
    "query": "What differentiates each stage in multi-stage RL training setups?",
    "papers": [
      "2501.12948",
      "2504.13828",
      "2505.00551",
      "2505.17667",
      "2512.20491",
      "2504.07128",
      "2502.21321",
      "2509.08827"
    ]
  },
  {
    "query": "Other than MoE, what architecture changes to the Transformer are used in training frontier LLMs today?",
    "papers": [
      "2412.19437",
      "2503.19786",
      "2504.03624",
      "2410.05258",
      "2504.00927",
      "2510.26692",
      "2504.06261",
      "2505.09343"
    ]
  },
  {
    "query": "What are good math benchmarks for evaluating an LLM's ability to do math reasoning?",
    "papers": [
      "2503.24235",
      "2501.12948",
      "2402.03300",
      "2504.11456",
      "2504.16074",
      "2503.21934",
      "2503.09567",
      "2502.21321",
      "2504.07086"
    ]
  },
  {
    "query": "Are there any multimodal architectures that use the next-token-prediction paradigm for generating images, instead of diffusion?",
    "papers": [
      "2505.14683",
      "2503.16430",
      "2504.17789",
      "2404.02905",
      "2408.11039",
      "2504.11455",
      "2503.22020",
      "2504.02949",
      "2505.09568"
    ]
  },
  {
    "query": "In what situations is an SFT phase commonly used between pre-training and RL and when is it excluded?",
    "papers": [
      "2501.12948",
      "2501.17161",
      "2503.24290",
      "2503.18892",
      "2502.01456",
      "2502.21321",
      "2503.06749",
      "2505.00551"
    ]
  },
  {
    "query": "How do VLA models generate actions from vision-language features?",
    "papers": [
      "2406.09246",
      "2503.14734",
      "2503.10631",
      "2503.22020",
      "2505.06111",
      "2504.16054",
      "2410.07864",
      "2508.21112",
      "2506.21539",
      "2303.04137"
    ]
  },
  {
    "query": "Do today's SOTA VLA models support cross-robot-platform action spaces? If so, how?",
    "papers": [
      "2505.06111",
      "2410.07864",
      "2510.10274",
      "2503.14734",
      "2406.09246",
      "2504.16054",
      "2405.12213",
      "2503.19757"
    ]
  },
  {
    "query": "Is dropout used when training modern state-of-the-art LLMs?",
    "papers": [
      "2312.00752",
      "2504.03624",
      "2212.14034",
      "2503.22329",
      "2509.25149",
      "2503.02656",
      "2505.24788",
      "2510.04071"
    ]
  },
  {
    "query": "How are researchers addressing the problem of models even from different families producing homogenous, non-diverse content?",
    "papers": [
      "2510.22954",
      "2501.18101",
      "2510.01171",
      "2504.13837",
      "2504.14945",
      "2503.24235",
      "2406.04692",
      "2509.15207",
      "2504.12522",
      "2510.20817"
    ]
  },
  {
    "query": "What scale of reward should I provide in RLVR for LLM-finetuning with PPO? Especially when introducing something like format rewards or additional signal besides binary",
    "papers": [
      "2511.13016",
      "2510.07242",
      "2503.06749",
      "2506.03637",
      "2501.12948",
      "2503.14476",
      "2504.20571",
      "2504.11343"
    ]
  },
  {
    "query": "In attention-based architectures and models, where are the common placements of the normalization layer within an attention block?",
    "papers": [
      "2002.04745",
      "2503.04598",
      "2409.19606",
      "2504.07866",
      "2408.00118",
      "2502.05795",
      "2111.09883",
      "2203.00555",
      "2503.10622"
    ]
  },
  {
    "query": "What are some continual learning strategies that actually involve updating weights at test-time instead of providing scaffolding?",
    "papers": [
      "2501.00663",
      "2407.04620",
      "2310.13807",
      "2504.07952",
      "2506.10943",
      "2512.23675",
      "2504.13173",
      "2302.00487",
      "2505.23735",
      "1703.03400"
    ]
  },
  {
    "query": "What are works that fine-tune video models specifically for use as VLA control models?",
    "papers": [
      "2406.09246",
      "2307.15818",
      "2410.24164",
      "2503.14734",
      "2410.07864",
      "2503.20020",
      "2504.02792",
      "2503.10631",
      "2503.22020",
      "2508.05635"
    ]
  },
  {
    "query": "When RL fine-tuning a retrieval agent that has access to tools, how should I shape the rewards? Are outcome-based rewards sufficient or do I need to add process-oriented rewards centered around tool-query quality, etc?",
    "papers": [
      "2504.13958",
      "2507.22844",
      "2505.11821",
      "2504.04736",
      "2505.01441",
      "2503.09516",
      "2509.02547",
      "2510.01132",
      "2504.11536",
      "2509.11963"
    ]
  },
  {
    "query": "What is the largest open-source LLM released in terms of parameter count?",
    "papers": [
      "2507.20534",
      "2412.19437",
      "2508.06471",
      "2407.21783",
      "2101.03961",
      "2505.09343",
      "2509.08827",
      "2405.04434"
    ]
  },
  {
    "query": "Is DPO superior to PPO for LLM Alignment?",
    "papers": [
      "2305.18290",
      "2402.14740",
      "2402.03300",
      "2405.14734",
      "2503.12854",
      "2501.12948",
      "2503.01067",
      "2504.13837",
      "2503.22230"
    ]
  },
  {
    "query": "What architectures do foundation models for robotics use?",
    "papers": [
      "2503.14734",
      "2406.09246",
      "2504.16054",
      "2303.04137",
      "2503.22020",
      "2505.06111",
      "2503.10631",
      "2410.07864",
      "2503.15558",
      "2304.13705"
    ]
  },
  {
    "query": "What is a good pre-trained base model to fine-tune on top of for VLA tasks?",
    "papers": [
      "2406.09246",
      "2503.14734",
      "2504.16054",
      "2503.20020",
      "2503.10631",
      "2410.07864",
      "2505.06111",
      "2506.01844"
    ]
  },
  {
    "query": "Which, if any, popular open source models adopt sliding window attention?",
    "papers": [
      "2310.06825",
      "2503.19786",
      "2004.05150",
      "2406.07522",
      "2103.14030",
      "2508.09834",
      "2504.14992",
      "2312.03863"
    ]
  },
  {
    "query": "What are some tricks to stabilize training when RL fine-tuning a large MoE model?",
    "papers": [
      "2412.19437",
      "2503.14476",
      "2408.15664",
      "2512.02556",
      "2507.18071",
      "2504.05118",
      "2509.08827",
      "2505.00551"
    ]
  },
  {
    "query": "Which, if any, popular open source models train with FP8?",
    "papers": [
      "2412.19437",
      "2505.09343",
      "2504.03624",
      "2508.14444",
      "2506.07900",
      "2512.20856",
      "2509.25149",
      "2512.02556"
    ]
  },
  {
    "query": "Which paper argues that a successful alignment algorithm should use on-policy sampling and negative gradients?",
    "papers": ["2404.14367"]
  },
  {
    "query": "Have people tried using intermediate rewards with GRPO? I don't know how that would look like",
    "papers": [
      "2502.01456",
      "2504.04736",
      "2505.10425",
      "2507.07017",
      "2504.15275",
      "2503.14476",
      "2509.21154",
      "2504.14945"
    ]
  },
  {
    "query": "What is the impact of introducing negative gradients when doing alignment fine-tuning for LLMs?",
    "papers": [
      "2506.01347",
      "2503.14391",
      "2407.10490",
      "2504.11364",
      "2504.10185",
      "2503.03710",
      "2410.07163",
      "2402.08787",
      "2504.11343",
      "2310.10683"
    ]
  }
]
